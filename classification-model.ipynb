{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (1.22.4)\n",
      "Requirement already satisfied: pandas_profiling in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (3.2.0)\n",
      "Requirement already satisfied: openpyxl in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (3.0.10)\n",
      "Requirement already satisfied: sklearn in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: torch in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (1.8.1)\n",
      "Requirement already satisfied: transformers in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (4.20.1)\n",
      "Requirement already satisfied: iso18245 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: markupsafe~=2.1.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (1.8.1)\n",
      "Requirement already satisfied: visions[type_image_path]==0.7.4 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (0.7.4)\n",
      "Requirement already satisfied: seaborn>=0.10.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (0.11.2)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (3.1.2)\n",
      "Requirement already satisfied: missingno>=0.4.2 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (0.5.1)\n",
      "Requirement already satisfied: phik>=0.11.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (0.12.2)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (4.64.0)\n",
      "Requirement already satisfied: multimethod>=1.4 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (1.8)\n",
      "Requirement already satisfied: PyYAML>=5.0.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (6.0)\n",
      "Requirement already satisfied: htmlmin>=0.1.12 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (0.1.12)\n",
      "Requirement already satisfied: tangled-up-in-unicode==0.2.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (0.2.0)\n",
      "Requirement already satisfied: requests>=2.24.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (2.28.0)\n",
      "Requirement already satisfied: joblib~=1.1.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (1.1.0)\n",
      "Requirement already satisfied: matplotlib>=3.2.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (3.5.2)\n",
      "Requirement already satisfied: pydantic>=1.8.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas_profiling) (1.9.1)\n",
      "Requirement already satisfied: networkx>=2.4 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from visions[type_image_path]==0.7.4->pandas_profiling) (2.8.4)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from visions[type_image_path]==0.7.4->pandas_profiling) (21.4.0)\n",
      "Requirement already satisfied: imagehash in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from visions[type_image_path]==0.7.4->pandas_profiling) (4.2.1)\n",
      "Requirement already satisfied: Pillow in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from visions[type_image_path]==0.7.4->pandas_profiling) (9.1.1)\n",
      "Requirement already satisfied: et-xmlfile in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from sklearn) (1.1.1)\n",
      "Requirement already satisfied: typing-extensions in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from matplotlib>=3.2.0->pandas_profiling) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from matplotlib>=3.2.0->pandas_profiling) (1.4.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from matplotlib>=3.2.0->pandas_profiling) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from matplotlib>=3.2.0->pandas_profiling) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from requests>=2.24.0->pandas_profiling) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from requests>=2.24.0->pandas_profiling) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from requests>=2.24.0->pandas_profiling) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from requests>=2.24.0->pandas_profiling) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: PyWavelets in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from imagehash->visions[type_image_path]==0.7.4->pandas_profiling) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing required packages\n",
    "%pip install pandas numpy pandas_profiling openpyxl sklearn torch transformers iso18245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preliminary analysis of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_excel(\"data/dbb4c4ff1f31-CAC+2022_Training+Data+Set+New.xlsx\",dtype={'default_brand':str, 'qrated_brand':str , 'coalesced_brand':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(train_set)\n",
    "profile.to_file(\"info/profile_train_test_set.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_set.drop([\"cdf_seq_no\",\"Category\"], axis=1)\n",
    "y_train = train_set[\"Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[\"merchant_cat_code\"].fillna(0, inplace=True)\n",
    "\n",
    "# scale merchant_cat_code (range 0-9999) to 0-0.9999 \n",
    "x_train[\"merchant_cat_code\"] = x_train[\"merchant_cat_code\"]/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly assign classes \n",
    "y_pred_rand = np.random.randint(0,len(label_encoder.classes_)-1,len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           precision    recall  f1-score   support\n",
      "\n",
      "                   Communication Services       0.01      0.10      0.01       282\n",
      "                                Education       0.01      0.10      0.02       445\n",
      "                            Entertainment       0.27      0.10      0.15     11255\n",
      "                                  Finance       0.00      0.12      0.01       185\n",
      "            Health and Community Services       0.11      0.11      0.11      4157\n",
      "           Property and Business Services       0.03      0.12      0.05      1095\n",
      "                             Retail Trade       0.34      0.11      0.17     13500\n",
      "                    Services to Transport       0.06      0.11      0.07      2317\n",
      "Trade, Professional and Personal Services       0.14      0.11      0.13      5275\n",
      "                                   Travel       0.00      0.00      0.00      1489\n",
      "\n",
      "                                 accuracy                           0.11     40000\n",
      "                                macro avg       0.10      0.10      0.07     40000\n",
      "                             weighted avg       0.22      0.11      0.13     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check accuracy of the random assignment\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train_encoded, y_pred_rand, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple ML algo run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using only mCatCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mCat code as the only feature (since it is higly correlated with the target)\n",
    "x_selected = x_train[[\"merchant_cat_code\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.63 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', C=1, gamma=0.1)\n",
    "scores = cross_val_score(clf, x_selected, y_train_encoded, cv=5, n_jobs=-1)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "# import naive bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "scores = cross_val_score(clf, x_selected, y_train_encoded, cv=5, n_jobs=-1)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.63 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=1000, objective='multi:softprob')\n",
    "\n",
    "scores = cross_val_score(clf, x_selected, y_train_encoded, cv=5, n_jobs=2)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using mCatCode and payment category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of payment_category\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oe = OneHotEncoder()\n",
    "x_payment_cat_encoded = pd.DataFrame(oe.fit_transform(x_train[[\"payment_category\"]]).toarray(), columns=oe.categories_[0])\n",
    "x_selected = x_train[[\"merchant_cat_code\"]].join(x_payment_cat_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.63 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', C=1, gamma=0.1)\n",
    "scores = cross_val_score(clf, x_selected, y_train_encoded, cv=5, n_jobs=-1)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.63 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=1000, objective='multi:softprob')\n",
    "\n",
    "scores = cross_val_score(clf, x_selected, y_train_encoded, cv=5, n_jobs=2)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oe = OneHotEncoder()\n",
    "\n",
    "x_encoded = pd.DataFrame(oe.fit_transform(x_train[[\"payment_category\",\"sor\",\"db_cr_cd\",\"is_international\"]]).toarray(), columns=oe.get_feature_names())\n",
    "x_selected = x_train[[\"merchant_cat_code\",\"amt\"]].join(x_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.60 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', C=1, gamma=0.1)\n",
    "scores = cross_val_score(clf, x_selected, y_train_encoded, cv=5, n_jobs=-1)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using payCat, merCatCode, amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oe = OneHotEncoder()\n",
    "\n",
    "x_encoded = pd.DataFrame(oe.fit_transform(x_train[[\"payment_category\"]]).toarray(), columns=oe.get_feature_names())\n",
    "x_selected = x_train[[\"merchant_cat_code\",\"amt\"]].join(x_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.60 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', C=1, gamma=0.1)\n",
    "scores = cross_val_score(clf, x_selected, y_train_encoded, cv=5, n_jobs=-1)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect rows with mCatCode null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[train_set['merchant_cat_code'].isna()].sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT features extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 10,\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "desc = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger batch sizes tend to be better, and we can fit this in memory.\n",
    "batch_size = 32\n",
    "\n",
    "# I used a smaller learning rate to combat over-fitting that I was seeing in the\n",
    "# validation loss. I could probably try even smaller.\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Number of training epochs. \n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def maxLen(concatenated_brand):\n",
    "    len_list = []\n",
    "    # For every sentence...\n",
    "    for sent in concatenated_brand:\n",
    "        # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "        input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        len_list.append(len(input_ids))\n",
    "\n",
    "    # mean, max, min of len_list\n",
    "    print('Mean sentence length: ', np.mean(len_list))\n",
    "    print('Max sentence length: ', np.max(len_list))\n",
    "    print('Min sentence length: ', np.min(len_list))\n",
    "    plt.hist(len_list)\n",
    "    plt.xlabel('Sentence length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Sentence length distribution')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### default_brand, qrated_brand, coalesced_brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat default_brand, qrated_brand and coalesced_brand\n",
    "concatenated_brand = x_train[\"default_brand\"] + \" \" + x_train[\"qrated_brand\"] + \" \" + x_train[\"coalesced_brand\"]\n",
    "concatenated_brand.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sentence length:  13.2321\n",
      "Max sentence length:  61\n",
      "Min sentence length:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxLen(concatenated_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### default_brand, qrated_brand and coalesced_brand with mCatCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat default_brand, qrated_brand and coalesced_brand\n",
    "concatenated_brand = x_train[\"merchant_cat_code\"].astype(str) + \" \" + x_train[\"default_brand\"] + \" \" + x_train[\"qrated_brand\"] + \" \" + x_train[\"coalesced_brand\"]\n",
    "concatenated_brand.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  64\n",
      "Mean sentence length:  16.65485\n",
      "Max sentence length:  64\n",
      "Min sentence length:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxLen(concatenated_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merchant_cat_code,default_brand, qrated_brand and coalesced_brand and translated merchant_cat_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iso18245\n",
    "\n",
    "def convert(x):\n",
    "    try:\n",
    "        return iso18245.get_mcc(str(x).zfill(4)).iso_description\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "mcc_desc = x_train[\"merchant_cat_code\"].astype(int).apply(lambda x: convert(x) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat merchant_cat_code,default_brand, qrated_brand and coalesced_brand\n",
    "concatenated_brand = x_train[\"merchant_cat_code\"].astype(str) + \" \" + mcc_desc + \" \" + x_train[\"default_brand\"] + \" \" + x_train[\"qrated_brand\"] + \" \" + x_train[\"coalesced_brand\"]\n",
    "concatenated_brand.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen(concatenated_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "print('Encoding...')\n",
    "\n",
    "# For every sentence...\n",
    "for sent in concatenated_brand:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(y_train_encoded)\n",
    "\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training size: 32,000\n",
      "Validation size: 4,000\n",
      "      Test size: 4,000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# First, calculate the split sizes. 80% training, 10% validation, 10% test.\n",
    "train_size = int(0.8 * len(train_set))\n",
    "val_size = int(0.1 * len(train_set))\n",
    "test_size = len(train_set) - (train_size + val_size)\n",
    "\n",
    "# Sanity check the sizes.\n",
    "assert((train_size + val_size + test_size) == len(train_set))\n",
    "\n",
    "# Create a list of indeces for all of the samples in the dataset.\n",
    "indeces = np.arange(0, len(train_set))\n",
    "\n",
    "# Shuffle the indeces randomly.\n",
    "random.shuffle(indeces)\n",
    "\n",
    "# Get a list of indeces for each of the splits.\n",
    "train_idx = indeces[0:train_size]\n",
    "val_idx = indeces[train_size:(train_size + val_size)]\n",
    "test_idx = indeces[(train_size + val_size):]\n",
    "\n",
    "# Sanity check\n",
    "assert(len(train_idx) == train_size)\n",
    "assert(len(test_idx) == test_size)\n",
    "\n",
    "# With these lists, we can now select the corresponding dataframe rows using, \n",
    "# e.g., train_df = data_df.iloc[train_idx] \n",
    "\n",
    "print('  Training size: {:,}'.format(train_size))\n",
    "print('Validation size: {:,}'.format(val_size))\n",
    "print('      Test size: {:,}'.format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Split the samples, and create TensorDatasets for each split. \n",
    "train_dataset = TensorDataset(input_ids[train_idx], attention_masks[train_idx], labels[train_idx])\n",
    "val_dataset = TensorDataset(input_ids[val_idx], attention_masks[val_idx], labels[val_idx])\n",
    "test_dataset = TensorDataset(input_ids[test_idx], attention_masks[test_idx], labels[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples!)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,000.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,000.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,000.    Elapsed: 0:00:33.\n",
      "  Batch   160  of  1,000.    Elapsed: 0:00:44.\n",
      "  Batch   200  of  1,000.    Elapsed: 0:00:56.\n",
      "  Batch   240  of  1,000.    Elapsed: 0:01:07.\n",
      "  Batch   280  of  1,000.    Elapsed: 0:01:18.\n",
      "  Batch   320  of  1,000.    Elapsed: 0:01:30.\n",
      "  Batch   360  of  1,000.    Elapsed: 0:01:41.\n",
      "  Batch   400  of  1,000.    Elapsed: 0:01:53.\n",
      "  Batch   440  of  1,000.    Elapsed: 0:02:04.\n",
      "  Batch   480  of  1,000.    Elapsed: 0:02:16.\n",
      "  Batch   520  of  1,000.    Elapsed: 0:02:28.\n",
      "  Batch   560  of  1,000.    Elapsed: 0:02:40.\n",
      "  Batch   600  of  1,000.    Elapsed: 0:02:51.\n",
      "  Batch   640  of  1,000.    Elapsed: 0:03:03.\n",
      "  Batch   680  of  1,000.    Elapsed: 0:03:15.\n",
      "  Batch   720  of  1,000.    Elapsed: 0:03:27.\n",
      "  Batch   760  of  1,000.    Elapsed: 0:03:39.\n",
      "  Batch   800  of  1,000.    Elapsed: 0:03:51.\n",
      "  Batch   840  of  1,000.    Elapsed: 0:04:03.\n",
      "  Batch   880  of  1,000.    Elapsed: 0:04:15.\n",
      "  Batch   920  of  1,000.    Elapsed: 0:04:27.\n",
      "  Batch   960  of  1,000.    Elapsed: 0:04:39.\n",
      "\n",
      "  Average training loss: 1.08\n",
      "  Training epcoh took: 0:04:51\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.73\n",
      "  Validation Loss: 0.82\n",
      "  Validation took: 0:00:13\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,000.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  1,000.    Elapsed: 0:00:24.\n",
      "  Batch   120  of  1,000.    Elapsed: 0:00:36.\n",
      "  Batch   160  of  1,000.    Elapsed: 0:00:49.\n",
      "  Batch   200  of  1,000.    Elapsed: 0:01:01.\n",
      "  Batch   240  of  1,000.    Elapsed: 0:01:13.\n",
      "  Batch   280  of  1,000.    Elapsed: 0:01:25.\n",
      "  Batch   320  of  1,000.    Elapsed: 0:01:37.\n",
      "  Batch   360  of  1,000.    Elapsed: 0:01:48.\n",
      "  Batch   400  of  1,000.    Elapsed: 0:02:00.\n",
      "  Batch   440  of  1,000.    Elapsed: 0:02:12.\n",
      "  Batch   480  of  1,000.    Elapsed: 0:02:24.\n",
      "  Batch   520  of  1,000.    Elapsed: 0:02:36.\n",
      "  Batch   560  of  1,000.    Elapsed: 0:02:48.\n",
      "  Batch   600  of  1,000.    Elapsed: 0:03:00.\n",
      "  Batch   640  of  1,000.    Elapsed: 0:03:12.\n",
      "  Batch   680  of  1,000.    Elapsed: 0:03:24.\n",
      "  Batch   720  of  1,000.    Elapsed: 0:03:36.\n",
      "  Batch   760  of  1,000.    Elapsed: 0:03:48.\n",
      "  Batch   800  of  1,000.    Elapsed: 0:04:00.\n",
      "  Batch   840  of  1,000.    Elapsed: 0:04:12.\n",
      "  Batch   880  of  1,000.    Elapsed: 0:04:24.\n",
      "  Batch   920  of  1,000.    Elapsed: 0:04:36.\n",
      "  Batch   960  of  1,000.    Elapsed: 0:04:48.\n",
      "\n",
      "  Average training loss: 0.75\n",
      "  Training epcoh took: 0:05:00\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.74\n",
      "  Validation Loss: 0.77\n",
      "  Validation took: 0:00:13\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,000.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  1,000.    Elapsed: 0:00:24.\n",
      "  Batch   120  of  1,000.    Elapsed: 0:00:36.\n",
      "  Batch   160  of  1,000.    Elapsed: 0:00:47.\n",
      "  Batch   200  of  1,000.    Elapsed: 0:00:59.\n",
      "  Batch   240  of  1,000.    Elapsed: 0:01:11.\n",
      "  Batch   280  of  1,000.    Elapsed: 0:01:23.\n",
      "  Batch   320  of  1,000.    Elapsed: 0:01:35.\n",
      "  Batch   360  of  1,000.    Elapsed: 0:01:46.\n",
      "  Batch   400  of  1,000.    Elapsed: 0:01:58.\n",
      "  Batch   440  of  1,000.    Elapsed: 0:02:11.\n",
      "  Batch   480  of  1,000.    Elapsed: 0:02:23.\n",
      "  Batch   520  of  1,000.    Elapsed: 0:02:35.\n",
      "  Batch   560  of  1,000.    Elapsed: 0:02:47.\n",
      "  Batch   600  of  1,000.    Elapsed: 0:02:59.\n",
      "  Batch   640  of  1,000.    Elapsed: 0:03:11.\n",
      "  Batch   680  of  1,000.    Elapsed: 0:03:23.\n",
      "  Batch   720  of  1,000.    Elapsed: 0:03:35.\n",
      "  Batch   760  of  1,000.    Elapsed: 0:03:47.\n",
      "  Batch   800  of  1,000.    Elapsed: 0:03:58.\n",
      "  Batch   840  of  1,000.    Elapsed: 0:04:10.\n",
      "  Batch   880  of  1,000.    Elapsed: 0:04:22.\n",
      "  Batch   920  of  1,000.    Elapsed: 0:04:34.\n",
      "  Batch   960  of  1,000.    Elapsed: 0:04:46.\n",
      "\n",
      "  Average training loss: 0.64\n",
      "  Training epcoh took: 0:04:58\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.75\n",
      "  Validation Loss: 0.78\n",
      "  Validation took: 0:00:13\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,000.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  1,000.    Elapsed: 0:00:24.\n",
      "  Batch   120  of  1,000.    Elapsed: 0:00:36.\n",
      "  Batch   160  of  1,000.    Elapsed: 0:00:48.\n",
      "  Batch   200  of  1,000.    Elapsed: 0:01:00.\n",
      "  Batch   240  of  1,000.    Elapsed: 0:01:12.\n",
      "  Batch   280  of  1,000.    Elapsed: 0:01:24.\n",
      "  Batch   320  of  1,000.    Elapsed: 0:01:36.\n",
      "  Batch   360  of  1,000.    Elapsed: 0:01:48.\n",
      "  Batch   400  of  1,000.    Elapsed: 0:02:00.\n",
      "  Batch   440  of  1,000.    Elapsed: 0:02:12.\n",
      "  Batch   480  of  1,000.    Elapsed: 0:02:24.\n",
      "  Batch   520  of  1,000.    Elapsed: 0:02:36.\n",
      "  Batch   560  of  1,000.    Elapsed: 0:02:48.\n",
      "  Batch   600  of  1,000.    Elapsed: 0:03:00.\n",
      "  Batch   640  of  1,000.    Elapsed: 0:03:12.\n",
      "  Batch   680  of  1,000.    Elapsed: 0:03:24.\n",
      "  Batch   720  of  1,000.    Elapsed: 0:03:36.\n",
      "  Batch   760  of  1,000.    Elapsed: 0:03:48.\n",
      "  Batch   800  of  1,000.    Elapsed: 0:04:00.\n",
      "  Batch   840  of  1,000.    Elapsed: 0:04:12.\n",
      "  Batch   880  of  1,000.    Elapsed: 0:04:23.\n",
      "  Batch   920  of  1,000.    Elapsed: 0:04:35.\n",
      "  Batch   960  of  1,000.    Elapsed: 0:04:47.\n",
      "\n",
      "  Average training loss: 0.58\n",
      "  Training epcoh took: 0:04:59\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.75\n",
      "  Validation Loss: 0.79\n",
      "  Validation took: 0:00:13\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:20:40 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.078342</td>\n",
       "      <td>0.823186</td>\n",
       "      <td>0.73075</td>\n",
       "      <td>0:04:51</td>\n",
       "      <td>0:00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.750434</td>\n",
       "      <td>0.772836</td>\n",
       "      <td>0.74250</td>\n",
       "      <td>0:05:00</td>\n",
       "      <td>0:00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.644952</td>\n",
       "      <td>0.778967</td>\n",
       "      <td>0.74725</td>\n",
       "      <td>0:04:58</td>\n",
       "      <td>0:00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.580174</td>\n",
       "      <td>0.792017</td>\n",
       "      <td>0.74725</td>\n",
       "      <td>0:04:59</td>\n",
       "      <td>0:00:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1           1.078342     0.823186        0.73075       0:04:51         0:00:13\n",
       "2           0.750434     0.772836        0.74250       0:05:00         0:00:13\n",
       "3           0.644952     0.778967        0.74725       0:04:58         0:00:13\n",
       "4           0.580174     0.792017        0.74725       0:04:59         0:00:13"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "# pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap (doesn't seem to work in Colab).\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 4,000 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader to batch our test samples for us. We'll use a sequential\n",
    "# sampler this time--don't need this to be random!\n",
    "prediction_sampler = SequentialSampler(test_dataset)\n",
    "prediction_dataloader = DataLoader(test_dataset, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_dataset)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      result = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     return_dict=True)\n",
    "\n",
    "  logits = result.logits\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate the F1\n",
    "f1 = f1_score(flat_true_labels, flat_predictions, average='micro')\n",
    "\n",
    "print('F1 Score: %.3f' % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertTextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This takes a transformer backbone and puts a slightly-modified classification head on top.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, numeric_feature_size, num_labels):\n",
    "        # num_extra_dims corresponds to the number of extra dimensions of numerical/categorical data\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_labels = num_labels\n",
    "        self.config = BertConfig.from_pretrained(model_name)\n",
    "        \n",
    "        self.transformer = BertModel.from_pretrained(model_name, config=self.config)\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size + numeric_feature_size, num_labels)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, extra_data, labels, attention_mask=None):\n",
    "        \"\"\"\n",
    "        extra_data should be of shape [batch_size, dim] \n",
    "        where dim is the number of additional numerical/categorical dimensions\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask) # [batch size, sequence length, hidden size]\n",
    "        cls_embeds = outputs.last_hidden_state[:, 0, :] # [batch size, hidden size]\n",
    "        concat = torch.cat((cls_embeds, extra_data.float()), dim=-1) # [batch size, hidden size+num extra dims]\n",
    "\n",
    "        pooled_output = self.dropout(concat)\n",
    "        \n",
    "        logits = self.classifier(pooled_output) # [batch size, num labels]    \n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return {\n",
    "            'loss':loss,\n",
    "            'logits':logits,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model \n",
    "model = BertTextClassifier(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    numeric_feature_size = 1,\n",
    "    num_labels = 10,\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "desc = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger batch sizes tend to be better, and we can fit this in memory.\n",
    "batch_size = 32\n",
    "\n",
    "# I used a smaller learning rate to combat over-fitting that I was seeing in the\n",
    "# validation loss. I could probably try even smaller.\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Number of training epochs. \n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def maxLen(concatenated_brand):\n",
    "    len_list = []\n",
    "    # For every sentence...\n",
    "    for sent in concatenated_brand:\n",
    "        # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "        input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        len_list.append(len(input_ids))\n",
    "\n",
    "    # mean, max, min of len_list\n",
    "    print('Mean sentence length: ', np.mean(len_list))\n",
    "    print('Max sentence length: ', np.max(len_list))\n",
    "    print('Min sentence length: ', np.min(len_list))\n",
    "    plt.hist(len_list)\n",
    "    plt.xlabel('Sentence length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Sentence length distribution')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### default_brand, qrated_brand, coalesced_brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat default_brand, qrated_brand and coalesced_brand\n",
    "concatenated_brand = x_train[\"default_brand\"] + \" \" + x_train[\"qrated_brand\"] + \" \" + x_train[\"coalesced_brand\"]\n",
    "concatenated_brand.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sentence length:  19.0444\n",
      "Max sentence length:  77\n",
      "Min sentence length:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAEtCAYAAAB6YXVDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsiElEQVR4nO3deZgcVbnH8e+EJYkmIMRcDKAgiy/gFlklICYKeEE2WQTBYPAiCqLsIIIQVMCwSNiCbBKCGyFiwvJwQVZlh0DYeVkTxYRLCFsCCYFk7h/vaVJUume6Z6ane2p+n+eZp2aqTp06dbqn3z5V59RpaW1tRUREpGj6NLoAIiIi9aAAJyIihaQAJyIihaQAJyIihaQAJyIihaQAJyIihaQAJ9IAZrammbWa2ehGl6UWZjY8lXtUnfIfb2at7a2rt2YpRyOPWwTLNroA0nzMbC3gZ8BWwKeAd4GXgfuB8e5+WzeUYTgwHBjr7m/U+3iyhJmtCYwCJrv7tIYWphN60nmkLwwfc/exDS5KoagFJx9iZhsDjwHfBm4CDgNOAm4FhgG7dVNRhgMnAh/rpuPJEmsSdT+0scX4wA+A/h3Yb006fh4dPWZHjQIOrbCtu8tSGGrBSd6JwEeAoe7+SH6jmX2i+4skvZm7vwe8V+/jmFkL8FF3n9ddx6xGM5Wlp1GAk7x1gTnlghuAu7+cX2dmWwNHA5sC/YBngHHu/rtcuunAdOBHwJnEJdDFwN+Bg0t5m9l44HtptxfNrJTFSe4+OqVZEfg50aL8JPAWcDNwnLu/kDnmKOAy4OvAhsCBwOrADOBkd7+8zPmMAI4Evgx8FJgJ3AYc4+6vZtLtCfwE+CKwDNHyPd3dJ5Wru2pVm2+6L3M5cCHwG2BjYAHwN+BQd5+XS//VlG4o8CZwJXAR8DipbjP1BXCZmZV+v8Pdh+fy24+op3WIS9jnu/tpVZ5jP+BXwD7ASukcj6+QdjzwPXdvyaz7JHFl4evAJ9L5PAdc6O6Xt3ce6RL4bcB+xGv8Y2Bt4FRgdLljZo49mHj/fpN4v98LHOXuD2XSfJC/u49v63zS/8Ua6ffsvbYR7n57pbKY2RdSHWyVzuEFYDxwprsvyh+PuBryG+J/ZgVgKnC4u9+XP8ei0CVKyXseGGRmu1aT2MwOIC5lDgBOBg5PeVxgZqeX2WU14HbgX8BRwJ+AXYEJmTQXEh/SEJdIR6afq9MxVwTuBg4CrieCwXnA14D7zGyNMsc9JeVxIRGMFwPjzWyL3Pn8ELgF+AJwQcr7j8BGRGAspfs18BdgLvAL4p7lO8BVZvbj8rXVvg7kOxS4DniAqPubgP8BfpvLd8u07dPEh9ypREDMB/h/EHUFEfxKdX9yLt2PgBOAPwNHALOAMWa2d5Wn+mciOD6YlncSr+9G7e1oZssSX4r2IOrqoHROzwBfqfE8DiXq+C/Ea13Nh/3/AkOA0cBYoh7vMLPPVbFvOYcCTwOvZso5Eniq0g7pVsI9wAjgd8T/0kvAGD78v5R1I/Ee/iXx+n8OuN7MBnaw3E1PLTjJ+zWwDfBXM3uW+OB5ALjd3T/0D2dmQ4BzgL+4e/aDbZyZnQ0cbmYXZFtUxLf9Pd19YiafxcBBZmYe7jGzR4FvER0EpufK+EtgLeDL2ZZm+qb6GPGtdlRun77AJu6+MKWdRHzjPRi4K61bPZ3P08CwXOeWX5hZn5RuQ+A44FR3/3kmzTlmNhk41cwmuPtcatDBfL8AbJ75Fn6hma0A7Gdmh2dacb8FWtN5vZCON474svEBd3/BzP5OtI7vcfc/VCjup4D13f3NlNfviVbxT4gvLW2d57bALsDl7j4qs/4fLPli05YNACNa1GVbjDWex3ru/koVxy2ZAezm7q2p3FcT/yNnAP9dQz6lsk42s0OB/m2UM+9s4j29ubs/mspxHtEq39vMfu/ut+T2ecjdDyr9YWZPAhOBvYkvfoWjFpx8iLvfQ3yLvhxYkbiEMw540sz+kXpYluxO/JNdamYfz/4A1xLvr61zh5iZDW7JrWm5bnvlS/dJ9iG+of8nd8y3ictF25bZdVwpuKXz/A/xjT97zD2A5YnLdW/kM3D3xenXfYhgcXmZ874GGAhs3t65lNGRfO8pc4npVuLL65oAZrYKsAkwJftlI93bObsD5QS4rBTcUl7vEHXf7mtIBDeAD7Xw3X0y4FXsXzruCDP7ryrSt2VCjcEN4LRScANw96lEi3JrMxvQyfK0K53zMOCaUnBL5WhlSQv1W2V2PSv3d9X/dz2VWnCyFHd/jNQCSpf7vgrsT1z+mWJmG6VgsX7a5eY2slsl9/cLZdLMSctBVRRvcEq3LTC7QprFZdZVOm72cmbpH/3hdsqwPtBCtPQqyZ93NTqSbzX1+em0LBc8qgko5VQ6bjWv4VrEa/RMmW1PEa2zitx9hpmdDBwLzDKzacRl5avc/YEqjp9VrgztKXfp8EniPbkG8EQH8qxF6fUsd5yniLpdq8y2D71m7j4n3d+u5jXrkRTgpE3uPgOYYGZXAP8EtiA6k9xJfBgD7Evcgykn/0G4qGyqsNQN/TbS3Ezcb6hWpeNWc8xy+7QC27WRb0c+5DqSb2frs6PaOm7dufvx6bLoN4kvXvsDR5nZae5+TA1ZvVOXAsbrWElDPnezHU9y6vk+aSgFOKmKu7ea2X1EgFstrX42LV9197ZacR1R6QNiNvAGsEIdjln6Nj+Utr/ZP0vca/lX/r5kJ9Ur3+lpWa5lVG5dvZ+a8QJx+fozLB2w1186eXnpcuu5wLmpV+aNwNFmdma67Fiv81ifuBybtQER9Gekv19Ly5XL7F+udVVLWV9My8+W2bYeUbflWti9ju7ByYeY2Tapl1p+fX+W3Nt6Mi0nEk85OSltz++zopn17WBRSp0jPvQBke6D/RHY1Mx2L7djJ+7LTAIWAiemjhr5fEvfdK9Iy1PMbJky6TpyebJu+abhFw8CO2fvoZrZcsAhZXYpW/ddaEpaHpVdaWa70M7lyZRuxVT2D7j7ApZcOlwpLet1Hkdn3gulzkFbA7dkOvW8CLxP7h60mQ0jhp/kzQNWyuZbSQredwM7Zntupn2PTX9W01mn8NSCk7yziGEC1xA9Et8hxpntTXzjnpDu0eHuL5nZgcAlwFPpMuYM4j7Z54nOBBuwpAVRi9I35DFm9kdifNfj7v440dNwC2CimU1MaRcS9z+2J8b3jKr1gOl8DgXOBx4zswnpfFYDdga+D0xz9wcsniE5GphmZlcRY+WGEB10tic6q9R6/LrkmxxJdIS4O/WefJN4Wk0pv2wL4klimMJBZvYO0WJ+xd1vpQu4+41mdi3wPTNbmeh2vzbwQ2JMXnvd7UcAF5nZX4l7iPOI+tkfuM/dS/cV63UeawA3pv+RIURP3PlkAra7z0u9evc3sz8TvVXXJTptPUqMccy6F9gBOM/M7iZag7e20QHmEOAO4J9mdj4xDnEH4BvAn8r0oOyV1IKTvMOJ8UhfJj5oLwJ+SnzQ/g/xD/oBd7+MGGj6MPEBNY7oKj6EGMe11MDwarj7XcAxxAffxcS4qd3TtjeJAHcicZnmVOJ+3E7EB8UFHTlmyvsC4jLhM8R5n0cMkp0K/DuT7iTiA2UmMY7pfOAAolfpTztx/HrlewdxXtOJrvPHEq26g1OS+Zm084G9iMHzY4m6P6Gjx65gT2LowqbEoOmvEOMhp1ax7yPEe3Q4MWTk7PT7KcQHPFDX8/hv4P+I4SiHpTJ/NdujMTkMuJS48nEWsBmwIzCtTJ5nAb8n3uMTUlk3qFQAd3+Q6El5BzEO8Ewi8B5D3BMXoKW1VQ+pFumtzGw34tLsd9z9L40uj0hXUgtOpBcws5bUESO7bjmixf4+uQHfIkWge3AivUNfYEa6n+nE2Kc9iSehjPEyzxgV6ekU4ER6h/eI53buTNwfbSEC3Y/dfVwjCyZSL7oHJyIihaQWXPfpSzwPcBYNfgqEiEgPsgxx1eEBYtxt1RTgus8mxKOuRESkdl8hHhFYNQW47jML4PXX32bx4qUvCw8aNIA5c+YttV6qpzrsGqrHzlMddl6pDvv0aWGllT4KlZ93W5ECXPdZBLB4cWvZAFfaJp2jOuwaqsfOUx12Xq4Oa761o3FwIiJSSApwIiJSSApwIiJSSApwIiJSSA3rZGJmmxBTmowgnoI9h5jj6Hh3fy6XdhhwGrAh8WTwK4Fj3f2dXLq+xNPFRxJzQj0CHFdu6oh65CkiIs2jkS24Y4jpMW4m5ja6iJjy4mEz+2BWXzMbCtwC9CMeDHsJMS3LlWXyHE9MUfGHlOdi4AYz2zybqB55iohIc2nkMIHfAnu7+8LSCjO7kphk8xiWTFh5CtG6G16aLdfMpgMXm9nXSpMXmtmmxNxPh7n72LRuAjGB4hhizjLqmKeIiDSRhgU4d7+7zLpnzewJYH0AM1sB2AY4PTMVPMSEgGcRMxKXZufdnXig7CWZ/BaY2aXAyWY2xN1n1SPPDleCtGngCv3p17e2t+jgwQO75NgL3n2fuW/Nbz+hiDStphrobWYtwCrEfS6AzxNlfDCbzt0Xmtk04EuZ1V8Cns4FLYD7iSenDyVGwtcjT6mDfn2XZccjpjTk2NeeuTNzG3JkEekqzdaLch9gNWBi+ntIWpYLIrOAVTN/D2kjHZm09chTRESaTNO04MxsPeB84mGaV6TV/dOy3BOkF2S2l9JWSpfNqx55Vm3QoAEVt3XV5TXpGr359ejN595VVIed19k6bIoAZ2afICZjfB3Yw90Xp02lmyB9y+zWL7O9lLZSumxe9cizanPmzCv7jLrBgwcye7YuimU1+gOit74eei92nuqw80p12KdPS5sNg7Y0PMCZ2YrADcCKwBbu/nJmc+lS4JCldox1M3NpK6Ujk7YeeYqISJNp6D04M+sHXAt8BtjB3T2X5HHgfWDj3H7LEx08pmVWTwPWM7N8qN8sLUsdV+qRp4iINJmGBTgzW4YYWL05cVny3nwad3+TGAg+MhdkRgIDgKsy6yYBywH7Z47RF9gPuMvdZ9YrTxERaT6NvER5JrAT0YJb2cy+m9k2z90np9+PIx7hdbuZXQKsDhwB3ODuN5d2cPf7zOwq4DQzGwI8D3yPeAzYqNyx65GniIg0kUYGuKFpuWP6yZoBTAZw94fMbGviySFnEc+NvBg4tkye+wK/SsuVgEeB7d39rmyieuQpIiLNpaW1VbPOdpM1gRfVi7J6gwcPbOhA7976eui92Hmqw84r04vy08D0WvJotoHeIiIiXUIBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECkkBTkRECmnZRh7czIYAhwCbARsDA4AR7n57Lt10YI0yWYxx95/l0n4MOA34FvAR4D7gcHefVub4OwGjgQ2AV4BLgZPd/f2O5ikiIs2h0S04A44BVgcebSftVGBk7ucvH8rMrA9wPbAXcC5wNLAKcLuZrZ1Lux0wGXgN+En6/QTgrI7mKSIizaOhLTgiaH3c3eeY2S7A39pI+5K7/6Gd/HYHhgHfcvfJAGY2EXgGOBHYN5P2DOBh4BvuviilfQs41szOcfdnO5CniIg0iYa24Nx9rrvPqTa9mfU1s4+0kWR3YCYwJXOM2cBEYBczWy7lswFxWfLCUnBLxhF1sluteYqISHNp9CXKWmwLvA28bWbPm9kBZdJ8CZjq7q259fcDA4F1MukAHswmcveZwEuZ7bXkKSIiTaSnBLhHicuBuwE/AF4FLjSzn+XSDQFmldm/tG7VTDraSLtq5u9q8xQRkSbS6HtwVXH3nbJ/m9llwJ3AL8zsAnd/M23qD7xbJosFme3ZZaW02cug1eZZlUGDBlTcNnjwwFqykjrrza9Hbz73rqI67LzO1mGPCHB57r7IzMYSvSg3B/43bZoP9C2zS7/M9uyyUtr5mb+rzbMqc+bMY/Hi/NXOeCFnz55bS1aF1+gPiN76eui92Hmqw84r1WGfPi1tNgza0lMuUZbz77RcObNuFksuP2aV1s3MpKONtDMzf1ebp4iINJGeHODWSsvZmXXTgI3MrCWXdjNgHvBcJh3E4PIPmNmqxJi8aZnV1eYpIiJNpOkDnJmtnAZbZ9f1A44C5gL3ZDZNIjp97JxJ+3FgD2CKu78H4O5PAE8DB5jZMpn9DwQWA3+tNU8REWkuDb8HZ2bHp1/XT8uRZrYl8Ia7nwfsBBxnZpOA6cAg4HvAZ4AD3X1eJrtJwL3ABDM7g+hteRARyEfnDn0UcA1wo5ldCXwOOJgYG/dMB/MUEZEm0fAAB/wq9/f303IGcB7wGNHaGgkMJno0PgQc4e7XZXdMnU+2B04Hfkr0cLwf2Nfdn8ulvc7MdiWGH5xLXOr8db48teQpIiLNo+EBzt3z97by26cCO9aQ3+vA/umnvbSTiWdQdlmeIiLSHJr+HpyIiEhHKMCJiEghKcCJiEghKcCJiEghKcCJiEghKcCJiEghKcCJiEghKcCJiEghKcCJiEghKcCJiEghKcCJiEghKcCJiEghKcCJiEgh1RTgzOwFM9upje07mNkLnS+WiIhI59TaglsTGNDG9o8Ca3S4NCIiIl2kqy9RrgK808V5ioiI1KzdCU/NbCtgeGbVrma2TpmkKwN7AdO6pGQiIiKdUM2M3iOAE9PvrcCu6aec54DDuqBcIiIinVJNgBsLjAdagBeAQ4EpuTStwDx3f60LyyYiItJh7QY4d38TeBPAzEYAT7n7K/UumIiISGdU04L7gLvfUa+CiIiIdKWaAhyAmX0K+CGwLjCIuHSZ1eruX++CsomIiHRYTQHOzLYD/gYsD8wD5tSjUCIiIp1VawvuVOBVYBd3f7AO5REREekStQ70Xg8Yq+AmIiLNrtYANxtYWI+CiIiIdKVaA9wVwG71KIiIiEhXqvUe3HhghJlNAc4GXgQW5RO5+786XzQREZGOqzXAPU08taQF2KGNdMt0uEQiIiJdoNYA90siwImIiDS1Wp9kMrpO5RAREelSXT0fnIiISFOo9UkmW1WTzt3/0bHiiIiIdI1a78HdTnX34NTJREREGqrWALdfhTzWBkYB04ELO1ckERGRzqu1k8nllbaZ2enAQ50ukYiISBfosk4m7v46cAlwdFflKSIi0lE1zwfXjteBtapNbGZDgEOAzYCNgQHACHe/vUzanYDRwAbAK8ClwMnu/n4u3ceA04BvAR8B7gMOd/dp3ZGniIg0hy5rwZlZP2Ak8HItuwHHAKsDj7aR93bAZOA14Cfp9xOAs3Lp+gDXA3sB5xKtyVWA281s7XrnKSIizaPWYQK/r7BpZWBzYDBwVA1ZTgU+7u5zzGwXYjLVcs4AHga+4e6LUlneAo41s3Pc/dmUbndgGPAtd5+c0k0EngFOBPatc54iItIkam3BjarwsyXwPPBddz+z2szcfa67tzkruJltQFxCvLAUiJJxRPmzsxvsDswEpmSOMRuYCOxiZsvVK08REWkutfaibMSTT76Ulh+aZNXdZ5rZS5ntpbRT3T0/Vu9+4ABgHeCpOuUpIiJNpCc8qmtIWs4qs20WsGoubaV0ZNLWI08REWkiHepFaWYrAFuzpMfkC8Df3X1uVxUso39avltm2wKiV2M2baV02bzqkWdVBg0aUHHb4MEDa8lK6qw3vx69+dy7iuqw8zpbhzUHODPbHziT6NLfkla3AvPM7HB3v7RTJVra/LTsW2Zbv8z2UtpK6bJ51SPPqsyZM4/Fi5d+2tngwQOZPbse3w96rkZ/QPTW10Pvxc5THXZeqQ779Glps2HQlpouUaZxYxcBs4HDgG3Sz2HEOLKLzGzHDpWkstKlwCFltg0hOoBk01ZKRyZtPfIUEZEmUmsL7miiQ8Vm7j4vs/4WM7sMuJcY13ZtF5UPYFpabkzmUWBmtioxfm5aLu0wM2vJdQrZDJgHPFfHPEVEpInU2snki8D4XHADoss/cHlK02Xc/QngaeAAM8vOUnAgsBj4a2bdJKLTx86lFWb2cWAPYIq7v1evPEVEpLnU2oJraWd7NVPpfIiZHZ9+XT8tR5rZlsAb7n5eWncUcA1wo5ldCXwOOJgYx/ZMJrtJRCtygpmdAbwKHEQE8tG5Q9cjTxERaRK1tuAeAUaZ2UfzG8xsADHo+5Ea8/xV+tk7/f399PeRpQTufh2wKzCIeFzWrsCvgZ9mM0qDtrcnBmH/FDiduF84wt2fy6Xt8jxFRKR51NqCOx24GnjIzM4BnkzrP0s8z3EdIlBUzd3baxWW0k0mnhfZXrrXgf3TT7fnKSIizaHWJ5lMNrODgTFEq6d0SbIFeBs42N2nVNpfRESku9Q8Ds7dx5nZn4jhAZ9Oq0sDvd/sysKJiIh0VIeeZOLubwBXdW1RREREuk67AS51oz8ZmO7uv2sj3YHAJ4HjyjyYWEREpFtV04vyu0SX+gfaSXc/Mcj7O50tlIiISGdVE+C+Ddzs7lPbSpS234gCnIiINIFqAtxGwM1V5ncb8fgrERGRhqomwK1MPEi5GrNTehERkYaqJsDNBT5eZX6DiAcQi4iINFQ1Ae4JYNsq89smpRcREWmoagLc1cDWZrZzW4nSXHHb8OEn8YuIiDRENQO9LySmkZmYnqZ/sbtPL200szWJZzQeCTyT0kuBDFyhP/36duiZACIiDdPup5a7zzezbwLXAccCPzOzt4h7cwOBFYhnUTqwg7svqGN5pQH69V2WHY/o/keMXntmmxcNRETaVNV0OWlamKHAIcCdwCLgE2n5z7R+Q3d/vj7FFBERqU3V151Sy+zc9CMiItLUap3wVEREpEdQgBMRkUJSgBMRkUJSgBMRkUJSgBMRkUJSgBMRkUJSgBMRkUJSgBMRkUJSgBMRkUJSgBMRkUJSgBMRkULSHCgiZSx8bxGDBw/s9uMuePd95r41v9uPK1JECnAiZSy/3DINmyJobrcfVaSYdIlSREQKSQFOREQKSQFOREQKSQFOREQKSQFOREQKSQFOREQKSQFOREQKSQFOREQKSQFOREQKSQFOREQKqUc8qsvMhgO3Vdi8vrs/nUk7DDgN2BB4C7gSONbd38nl2Rf4JTASWAl4BDjO3W8pc/yq8hQRkebRIwJcxlhgam7dzNIvZjYUuAV4AjgcWB04ElgL2DG333hgt5Tnc8Ao4AYz+6q739PBPEVEpEn0tAB3h7tPbmP7KcAcYLi7zwMws+nAxWb2NXe/Na3bFNgLOMzdx6Z1E4DHgTHAVrXmKSIizaXH3YMzs4FmtlRgNrMVgG2ACaVAlEwA5gHfzqzbHXgPuKS0wt0XAJcCW5rZkA7kKSIiTaSnBbgriHtg883sJjP7fGbb54kW6YPZHdx9ITAN+FJm9ZeAp3NBC+B+oAUY2oE8RUSkifSUS5QLgUnADcCrwBeI+2B3mtkm7v4MMCSlnVVm/1nA5pm/hwD/qZAOYNVMumrzrMqgQQMqbmvEBJvSfJrhfdAMZejpVIed19k67BEBzt3vBu7OrLrGzK4lWlYnAvsA/dO2d8tksSCznfR7pXRk0taSZ1XmzJnH4sWtS60fPHggs2c351SX+kftXo1+HzTze7GnUB12XqkO+/RpabNh0JaedonyA+7+CHAz8PW0an5a9i2TvF9meyltpXTZvGrJU0REmkiPDXDJv4GV0++ly4hDyqQbQmY4QUpbKR2ZtLXkKSIiTaSnB7i1gNnp98eB94GNswnMbHmi08i0zOppwHpmlm/3bpaWj3QgTxERaSI9IsCZ2eAy67YERgA3Arj7m8Qly5G5wDUSGABclVk3CVgO2D+TX19gP+Aud5/ZgTxFRKSJ9IhOJsCVZvYO0dHkVeBzwAHp99GZdMelNLeb2SXEU0eOAG5w95tLidz9PjO7CjgtjXl7HvgesAbxRBNqzVNERJpLj2jBAZOBwURgOZ94xNafgE3c/V+lRO7+ELA10evxLOAHwMXAHmXy3Bc4Oy3PIVp027v7XdlENeYpIiJNoke04Nz9HCIIVZP2TmCLKtItAI5KP12Sp4iINI+e0oITERGpiQKciIgUkgKciIgUkgKciIgUkgKciIgUkgKciIgUUo8YJiAwcIX+9Ourl0tEpFr6xOwh+vVdlh2PmNKQY1975s4NOa6ISGfoEqWIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBSSApyIiBTSso0ugIgssfC9RQwePLAhx17w7vvMfWt+Q44tUg8KcCJNZPnllmHHI6Y05NjXnrkzcxtyZJH60CVKEREpJAU4EREpJAU4EREpJAU4EREpJAU4EREpJPWirIKZ9QV+CYwEVgIeAY5z91saWjAREalILbjqjAcOA/4AHAIsBm4ws80bWSgREalMLbh2mNmmwF7AYe4+Nq2bADwOjAG2alzpRESkErXg2rc78B5wSWmFuy8ALgW2NLMhjSqYiIhUphZc+74EPO3u83Lr7wdagKHArCryWQagT5+Wigna2gbwXyv1r+Iw9dGoY+ucu0/2MWHd+biwd999n3nzFnTb8bpLe//P0r4+fVqy9bhMrfu3tLa2dm2JCsbMHgf+4+7fyK3fAHgC2N/dL60iqy2Bf9ahiCIivcFXgDtr2UEtuPb1B94ts35BZns1HiBeoFnAoi4ol4hIb7AMMIT4DK2JAlz75gN9y6zvl9lejXep8duHiIgA8HxHdlInk/bNIr495JXWzezGsoiISJUU4No3DVjPzAbk1m+Wlo90b3FERKQaCnDtmwQsB+xfWpGebLIfcJe7qwUnItKE1IuyCmY2EdgFOIu4Fvw9YBNghLvf1cCiiYhIBepkUp19gV+l5UrAo8D2Cm4iIs1LLTgRESkk3YMTEZFCUoATEZFC0j24BtI8c9Uzs02AUcAIYA1gDnA3cLy7P5dLOww4DdgQeAu4EjjW3d/pzjL3BGZ2NDErxiPuPjS3TfVYQXo/jgaGEb2snwfOcvfxmTQ7pTQbAK8QD2g/2d3f7+biNiUzWxf4NbAF8fk3A5hA1OO7mXQdfh+qBddY49E8c9U6BtgVuJmoq4uA4cDDZrZ+KZGZDQVuIZ40czgxC8QPiX8KyTCzTwDHA2+X2TYU1WNZZrYdcBcR2H4BHEG8Lz+ZSzMZeA34Sfr9BKIndq9nZqsRD6zfDDiP+BycCpxKZuaWzr4P1YJrEM0zV7PfAnu7+8LSCjO7EniMCH6j0upTiNbd8NIMEGY2HbjYzL7m7rd2Z6Gb3G+AB4kvuh/LbVM9lmFmKxJfTC9w90PaSHoG8DDwDXdflPZ9CzjWzM5x92frXtjm9l3iPbeluz+R1l1kZv2Bvczs++7+Hp18H6oF1ziaZ64G7n53Nrildc8SMzqsD2BmKwDbABNy0xtNAOYB3+6m4ja99AXru8S34vw21WNlexMfzCcAmNlAM/vQvDhpppENgAtLwS0ZR3zm7tY9RW1qK6Tl/+XWv0x8Li7qivehAlzjVDPPnLQhfbCsAryaVn2euCrxYDZdCozTiDrv9VK9nQtc7u7TyiRRPVa2NfA0sL2Z/Zu4J/Samf3GzErzlZXqJ19/M4GX6N31V3JHWl5qZl80s0+a2T7ElZgx7r6YLngfKsA1zhDKT5RaWrdqN5alp9oHWA2YmP4utXor1avqNOxLtDCOr7Bd9VjZOsS9tvHpZzfgb8Rl8jNTGtVfO9z9JuL+5TZEsPoX0RdhjLuflJJ1uh51D65xumqeuV7JzNYDziemILoirS7VWaV67fV1amYDiXtvv3H3SjPRqx4rG0D0+PuZu49J665OD2M/yMx+Tfv195H6F7NHeBG4nfiCMAf4JnCSmc1299/RBe9DBbjG6ap55nqd1PvveuB1YI90OQOW1FmlelWdRqttIdFppxLVY2Wlc/9zbv0fgT2ATVH9tcvM9gIuBD6TeWD91WbWBzgjdSDrdD0qwDWO5pnrgNSL7QZgRWALd385s7nUIqlUr726TlPHpUOJS0OrmFlpUz9geTNbE3gT1WNbZgGfZenOEaW/V+LD9ZdvJQ8hxm/2dgcBU8vMxnINcR/ui3TB+1D34BpnGppnriZm1g+4FvgMsIO7ey7J48D7wMa5/ZYnOu1Mq38pm9oqwPLEMJQXMz+bET1RXyTuJakeK5ualqvl1q+elrNZUj/5+ls1pZuGrAIsU2b9cmm5LF3wPlSAaxzNM1eD1EPtSmBz4rLkvfk07v4mMeB2ZO6Lw0ji3slV3VHWJvYi8K0yP08A09PvE1SPbSqd+/+UVqReqfsTA+bvTeO6ngYOyPSsBDiQeJjDX7uprM3sGWBjM1s7t/47wCLg0a54H2o2gQbSPHPVM7OxxBNMrmVJr8mSee4+OaXbkLgE9DgxxnB14kkTt7n79t1V3p7EzG4HPpZ9VJfqsTIzu5z4kL0UeIjoHPFN4Gh3Pz2l2YG43HYr8cXsc8DBxNi4gxpR7mZiZlsRdfMq8SST14AdgO2A37n7gSldp96HasE11r7A2Wl5DtGi0zxz5Q1Nyx2JXpPZn7GlRO7+EDFW6V3ii8MPgIuJDgBSJdVjm34AnAx8g/j/XQf4USm4Abj7dcSj5QYRYw53JZ67+NNuL20Tcvd/EM/xfAj4MfE/vDZwLPFFoJSuU+9DteBERKSQ1IITEZFCUoATEZFCUoATEZFCUoATEZFCUoATEZFCUoATEZFCUoATEZFCUoATkS5jZmuaWauZjW50WWphZsNTuUc1uizSdTSbgPQaZrYW8DNgK+BTxNMRXiZmUR/v7rd1QxmGA8OBse7+Rr2PJ0uk2RJGAZMrzGQuBaMAJ72CmW0M3AG8B0wgHjDcH1gX2BaYC9Q9wBHB7URiNug3uuF4ssSaRN1PR0/07xUU4KS3OJGYSXmouy81FVGaRFVECkQBTnqLdYE55YIbQG7iVADMbGvgaGKW5n7EFB/j3P13uXTTiVbBj4AziUugi4G/AweX8jaz8cSMEQAvZiYcPcndR6c0KwI/B3YDPgm8RUwZcpy7v5A55ijgMuDrwIbEVCyrAzOAk9398jLnMwI4Evgy8FFiwsjbgGPc/dVMuj2BnxCTTi4DPAac7u6TytVdtarN18xagcuJGZ9/Q8wHtgD4G3Cou8/Lpf9qSjeUmLD1SuAi4gn0J7n76Ex9AVxmZqXf73D34bn89iPqaR3iEvb57n5aZ85dGkOdTKS3eB4YZGa7VpPYzA4AbiLmnToZODzlcYGZnV5ml9WA24F/AUcBfyKeID8hk+ZC4kMa4DBiypWRwNXpmCsSU4McBFxPBIPzgK8B95nZGmWOe0rK40IiGC8GxpvZFrnz+SFwC/AF4IKU9x+BjVgyWSdm9mvgL8Ql218Q9yzfAa4ysx+Xr632dSDfocB1wANE3d9EzMH221y+W6ZtnyaC3KlEQMwH+H8QdQUR/Ep1f3Iu3Y+AE4A/E9OyzALGmNnetZyvNAfNJiC9gpltTtyDWw54FriT+PC83d2fyqUdQkwOerW7753bdjYxnce6pRZVasGtAezp7hMzac8ngtV6pdnHU+/CE4FPu/v0MnkfAHw529JMge2xVJ5Rad0ookUyDdjM3Rem9asBL6S030nrVieC8/PAsHznFjPr4+6L09xbU4FT3f3nuTSTiUC7mrvPXbqGP0i3Zqq7bKu0pnxTC64V2Nzd78ukvZ64X7pSqRVnZvcTQXuDzOuxHPFlY1iuHMOJFut+7j4+V47StlnA+mmyTczsI0Sr+Dl337zSeUtz0iVK6RXc/R4z24j4Vr4dMXP6fgBm9k9gVOYS4O5AX+BSM/t4LqtriTm9tiZaAiUzs8EtuZUIcOsC3lb50qzQ+xAtjf/kjvs2cC/x4Z43rhTc0nn+x8yeSccs2QNYnviwfyOfgbsvTr/uQwSWy8uc9zXAzsSM6je1dS5ldCTfe7LBLbkV2J7oLPK4ma1CTBA8MXv51t3fS18WhtVYToDLSsEt5fWOmd2byic9jAKc9Bru/hjRTbzUKvoqsD/wFWCKmW2UgsX6aZeb28huldzfL5RJMyctB1VRvMEp3bbA7AppFpdZV+m42cuZpWD3cDtlWB9oAZ5uI03+vKvRkXyrqc9Pp2W5Lw9tfqFoQ6XjVvMaSpNRgJNeyd1nABPM7Argn8AWRGeSO4kPY4iZ1mdVyCL/QbiojcO1tLEtn+ZmYEwV6ds7bjXHLLdPK9HCrZTvE92Ub2frs6PaOq70MApw0qu5e6uZ3UcEuNXS6mfT8lV3b6sV1xGVbnrPJsbFrVCHYz6TlkMzv5fzLPDfwL/y9yU7qV75Tk9LK7Ot3Dp1OOhl1ItSegUz28bMlvpCZ2b9WXJv68m0nEg85eSktD2/z4pm1reDRSl1cV85uzLdB/sjsKmZ7V5uRzP7rw4ecxKwEDjRzFYok2+pRXRFWp5iZsuUSdeRy5N1yzcNv3gQ2Dk9paaU33LAIWV2KVv3UlxqwUlvcRYxTOAaokfiO8Q4s72BzwAT0j063P0lMzsQuAR4Kl3GnEHcJ/s8sAuwAUtaELW4Ny3HmNkfifFdj7v748BxREtyoplNTGkXEvfTtid6Io6q9YDpfA4FzgceM7MJ6XxWIzp4fB+Y5u4PpF6eo4FpZnYVMVZuCDGcYHuis0qtx69LvsmRxHjDu81sHDEO7tuZ/LKttieJYQoHmdk7RIv5FXe/tYPHlianFpz0FocT482+THzQXkT0hpxJjK/aL5vY3S8jBmw/DPwQGEeMHRtCjONaamB4Ndz9LuAYYG3gYmK81e5p25tEgDsR+CwxpmsMsBMR7C7oyDFT3hcQlwmfIc77PGLQ+VTg35l0JwE7EPVyKBEUDyB6lf60E8evV753EOc1nRggfyzRqjs4JZmfSTsf2IsYPD+WqPsTOnpsaX4aBycihWNmuxGXZr/j7n9pdHmkMdSCE5Eey8xazKxfbt1yRIv9fWLAt/RSugcnIj1ZX2BGup/pxHi1PYmnm4wp94xR6T0U4ESkJ3uPeG7nzsT90RYi0P3Y3cc1smDSeLoHJyIihaR7cCIiUkgKcCIiUkgKcCIiUkgKcCIiUkgKcCIiUkgKcCIiUkj/DzzzqGvHn6SLAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxLen(concatenated_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "print('Encoding...')\n",
    "\n",
    "# For every sentence...\n",
    "for sent in concatenated_brand:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(y_train_encoded)\n",
    "\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training size: 32,000\n",
      "Validation size: 4,000\n",
      "      Test size: 4,000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# First, calculate the split sizes. 80% training, 10% validation, 10% test.\n",
    "train_size = int(0.8 * len(train_set))\n",
    "val_size = int(0.1 * len(train_set))\n",
    "test_size = len(train_set) - (train_size + val_size)\n",
    "\n",
    "# Sanity check the sizes.\n",
    "assert((train_size + val_size + test_size) == len(train_set))\n",
    "\n",
    "# Create a list of indeces for all of the samples in the dataset.\n",
    "indeces = np.arange(0, len(train_set))\n",
    "\n",
    "# Shuffle the indeces randomly.\n",
    "random.shuffle(indeces)\n",
    "\n",
    "# Get a list of indeces for each of the splits.\n",
    "train_idx = indeces[0:train_size]\n",
    "val_idx = indeces[train_size:(train_size + val_size)]\n",
    "test_idx = indeces[(train_size + val_size):]\n",
    "\n",
    "# Sanity check\n",
    "assert(len(train_idx) == train_size)\n",
    "assert(len(test_idx) == test_size)\n",
    "\n",
    "# With these lists, we can now select the corresponding dataframe rows using, \n",
    "# e.g., train_df = data_df.iloc[train_idx] \n",
    "\n",
    "print('  Training size: {:,}'.format(train_size))\n",
    "print('Validation size: {:,}'.format(val_size))\n",
    "print('      Test size: {:,}'.format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "mcat = torch.tensor(x_train[[\"merchant_cat_code\"]].values)\n",
    "\n",
    "# Split the samples, and create TensorDatasets for each split. \n",
    "train_dataset = TensorDataset(input_ids[train_idx], attention_masks[train_idx], mcat[train_idx], labels[train_idx])\n",
    "val_dataset = TensorDataset(input_ids[val_idx], attention_masks[val_idx], mcat[val_idx], labels[val_idx])\n",
    "test_dataset = TensorDataset(input_ids[test_idx], attention_masks[test_idx], mcat[val_idx], labels[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples!)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of  1,000.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,000.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,000.    Elapsed: 0:00:33.\n",
      "  Batch   160  of  1,000.    Elapsed: 0:00:44.\n",
      "  Batch   200  of  1,000.    Elapsed: 0:00:55.\n",
      "  Batch   240  of  1,000.    Elapsed: 0:01:06.\n",
      "  Batch   280  of  1,000.    Elapsed: 0:01:18.\n",
      "  Batch   320  of  1,000.    Elapsed: 0:01:29.\n",
      "  Batch   360  of  1,000.    Elapsed: 0:01:40.\n",
      "  Batch   400  of  1,000.    Elapsed: 0:01:51.\n",
      "  Batch   440  of  1,000.    Elapsed: 0:02:03.\n",
      "  Batch   480  of  1,000.    Elapsed: 0:02:14.\n",
      "  Batch   520  of  1,000.    Elapsed: 0:02:25.\n",
      "  Batch   560  of  1,000.    Elapsed: 0:02:36.\n",
      "  Batch   600  of  1,000.    Elapsed: 0:02:48.\n",
      "  Batch   640  of  1,000.    Elapsed: 0:02:59.\n",
      "  Batch   680  of  1,000.    Elapsed: 0:03:10.\n",
      "  Batch   720  of  1,000.    Elapsed: 0:03:22.\n",
      "  Batch   760  of  1,000.    Elapsed: 0:03:33.\n",
      "  Batch   800  of  1,000.    Elapsed: 0:03:45.\n",
      "  Batch   840  of  1,000.    Elapsed: 0:03:56.\n",
      "  Batch   880  of  1,000.    Elapsed: 0:04:08.\n",
      "  Batch   920  of  1,000.    Elapsed: 0:04:19.\n",
      "  Batch   960  of  1,000.    Elapsed: 0:04:31.\n",
      "\n",
      "  Average training loss: 0.71\n",
      "  Training epcoh took: 0:04:42\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.69\n",
      "  Validation Loss: 1.00\n",
      "  Validation took: 0:00:12\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of  1,000.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,000.    Elapsed: 0:00:23.\n",
      "  Batch   120  of  1,000.    Elapsed: 0:00:34.\n",
      "  Batch   160  of  1,000.    Elapsed: 0:00:46.\n",
      "  Batch   200  of  1,000.    Elapsed: 0:00:57.\n",
      "  Batch   240  of  1,000.    Elapsed: 0:01:08.\n",
      "  Batch   280  of  1,000.    Elapsed: 0:01:20.\n",
      "  Batch   320  of  1,000.    Elapsed: 0:01:31.\n",
      "  Batch   360  of  1,000.    Elapsed: 0:01:42.\n",
      "  Batch   400  of  1,000.    Elapsed: 0:01:54.\n",
      "  Batch   440  of  1,000.    Elapsed: 0:02:05.\n",
      "  Batch   480  of  1,000.    Elapsed: 0:02:17.\n",
      "  Batch   520  of  1,000.    Elapsed: 0:02:28.\n",
      "  Batch   560  of  1,000.    Elapsed: 0:02:39.\n",
      "  Batch   600  of  1,000.    Elapsed: 0:02:51.\n",
      "  Batch   640  of  1,000.    Elapsed: 0:03:02.\n",
      "  Batch   680  of  1,000.    Elapsed: 0:03:13.\n",
      "  Batch   720  of  1,000.    Elapsed: 0:03:25.\n",
      "  Batch   760  of  1,000.    Elapsed: 0:03:36.\n",
      "  Batch   800  of  1,000.    Elapsed: 0:03:48.\n",
      "  Batch   840  of  1,000.    Elapsed: 0:03:59.\n",
      "  Batch   880  of  1,000.    Elapsed: 0:04:10.\n",
      "  Batch   920  of  1,000.    Elapsed: 0:04:22.\n",
      "  Batch   960  of  1,000.    Elapsed: 0:04:33.\n",
      "\n",
      "  Average training loss: 0.57\n",
      "  Training epcoh took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.70\n",
      "  Validation Loss: 0.99\n",
      "  Validation took: 0:00:12\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of  1,000.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,000.    Elapsed: 0:00:23.\n",
      "  Batch   120  of  1,000.    Elapsed: 0:00:34.\n",
      "  Batch   160  of  1,000.    Elapsed: 0:00:46.\n",
      "  Batch   200  of  1,000.    Elapsed: 0:00:57.\n",
      "  Batch   240  of  1,000.    Elapsed: 0:01:08.\n",
      "  Batch   280  of  1,000.    Elapsed: 0:01:20.\n",
      "  Batch   320  of  1,000.    Elapsed: 0:01:31.\n",
      "  Batch   360  of  1,000.    Elapsed: 0:01:43.\n",
      "  Batch   400  of  1,000.    Elapsed: 0:01:54.\n",
      "  Batch   440  of  1,000.    Elapsed: 0:02:06.\n",
      "  Batch   480  of  1,000.    Elapsed: 0:02:17.\n",
      "  Batch   520  of  1,000.    Elapsed: 0:02:28.\n",
      "  Batch   560  of  1,000.    Elapsed: 0:02:40.\n",
      "  Batch   600  of  1,000.    Elapsed: 0:02:51.\n",
      "  Batch   640  of  1,000.    Elapsed: 0:03:03.\n",
      "  Batch   680  of  1,000.    Elapsed: 0:03:14.\n",
      "  Batch   720  of  1,000.    Elapsed: 0:03:26.\n",
      "  Batch   760  of  1,000.    Elapsed: 0:03:37.\n",
      "  Batch   800  of  1,000.    Elapsed: 0:03:48.\n",
      "  Batch   840  of  1,000.    Elapsed: 0:04:00.\n",
      "  Batch   880  of  1,000.    Elapsed: 0:04:11.\n",
      "  Batch   920  of  1,000.    Elapsed: 0:04:23.\n",
      "  Batch   960  of  1,000.    Elapsed: 0:04:34.\n",
      "\n",
      "  Average training loss: 0.50\n",
      "  Training epcoh took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.70\n",
      "  Validation Loss: 1.04\n",
      "  Validation took: 0:00:12\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of  1,000.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,000.    Elapsed: 0:00:23.\n",
      "  Batch   120  of  1,000.    Elapsed: 0:00:34.\n",
      "  Batch   160  of  1,000.    Elapsed: 0:00:46.\n",
      "  Batch   200  of  1,000.    Elapsed: 0:00:57.\n",
      "  Batch   240  of  1,000.    Elapsed: 0:01:09.\n",
      "  Batch   280  of  1,000.    Elapsed: 0:01:20.\n",
      "  Batch   320  of  1,000.    Elapsed: 0:01:31.\n",
      "  Batch   360  of  1,000.    Elapsed: 0:01:43.\n",
      "  Batch   400  of  1,000.    Elapsed: 0:01:54.\n",
      "  Batch   760  of  1,000.    Elapsed: 0:03:29.\n",
      "  Batch   800  of  1,000.    Elapsed: 0:03:40.\n",
      "  Batch   840  of  1,000.    Elapsed: 0:03:51.\n",
      "  Batch   880  of  1,000.    Elapsed: 0:04:02.\n",
      "  Batch   920  of  1,000.    Elapsed: 0:04:13.\n",
      "  Batch   960  of  1,000.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epcoh took: 0:04:34\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.71\n",
      "  Validation Loss: 1.20\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of  1,000.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,000.    Elapsed: 0:00:22.\n",
      "  Batch   120  of  1,000.    Elapsed: 0:00:32.\n",
      "  Batch   160  of  1,000.    Elapsed: 0:00:43.\n",
      "  Batch   200  of  1,000.    Elapsed: 0:00:54.\n",
      "  Batch   240  of  1,000.    Elapsed: 0:01:05.\n",
      "  Batch   280  of  1,000.    Elapsed: 0:01:15.\n",
      "  Batch   320  of  1,000.    Elapsed: 0:01:26.\n",
      "  Batch   360  of  1,000.    Elapsed: 0:01:37.\n",
      "  Batch   400  of  1,000.    Elapsed: 0:01:48.\n",
      "  Batch   440  of  1,000.    Elapsed: 0:01:58.\n",
      "  Batch   480  of  1,000.    Elapsed: 0:02:09.\n",
      "  Batch   520  of  1,000.    Elapsed: 0:02:20.\n",
      "  Batch   560  of  1,000.    Elapsed: 0:02:31.\n",
      "  Batch   600  of  1,000.    Elapsed: 0:02:42.\n",
      "  Batch   640  of  1,000.    Elapsed: 0:02:52.\n",
      "  Batch   680  of  1,000.    Elapsed: 0:03:03.\n",
      "  Batch   720  of  1,000.    Elapsed: 0:03:14.\n",
      "  Batch   760  of  1,000.    Elapsed: 0:03:25.\n",
      "  Batch   800  of  1,000.    Elapsed: 0:03:36.\n",
      "  Batch   840  of  1,000.    Elapsed: 0:03:47.\n",
      "  Batch   880  of  1,000.    Elapsed: 0:03:58.\n",
      "  Batch   920  of  1,000.    Elapsed: 0:04:09.\n",
      "  Batch   960  of  1,000.    Elapsed: 0:04:20.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epcoh took: 0:04:31\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.71\n",
      "  Validation Loss: 1.27\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of  1,000.    Elapsed: 0:00:11.\n",
      "  Batch    80  of  1,000.    Elapsed: 0:00:23.\n",
      "  Batch   120  of  1,000.    Elapsed: 0:00:34.\n",
      "  Batch   160  of  1,000.    Elapsed: 0:00:45.\n",
      "  Batch   200  of  1,000.    Elapsed: 0:00:57.\n",
      "  Batch   240  of  1,000.    Elapsed: 0:01:08.\n",
      "  Batch   280  of  1,000.    Elapsed: 0:01:20.\n",
      "  Batch   320  of  1,000.    Elapsed: 0:01:31.\n",
      "  Batch   360  of  1,000.    Elapsed: 0:01:43.\n",
      "  Batch   400  of  1,000.    Elapsed: 0:01:54.\n",
      "  Batch   440  of  1,000.    Elapsed: 0:02:06.\n",
      "  Batch   480  of  1,000.    Elapsed: 0:02:17.\n",
      "  Batch   520  of  1,000.    Elapsed: 0:02:29.\n",
      "  Batch   560  of  1,000.    Elapsed: 0:02:40.\n",
      "  Batch   600  of  1,000.    Elapsed: 0:02:51.\n",
      "  Batch   640  of  1,000.    Elapsed: 0:03:03.\n",
      "  Batch   680  of  1,000.    Elapsed: 0:03:14.\n",
      "  Batch   720  of  1,000.    Elapsed: 0:03:25.\n",
      "  Batch   760  of  1,000.    Elapsed: 0:03:37.\n",
      "  Batch   800  of  1,000.    Elapsed: 0:03:48.\n",
      "  Batch   840  of  1,000.    Elapsed: 0:04:00.\n",
      "  Batch   880  of  1,000.    Elapsed: 0:04:11.\n",
      "  Batch   920  of  1,000.    Elapsed: 0:04:22.\n",
      "  Batch   960  of  1,000.    Elapsed: 0:04:34.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epcoh took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.71\n",
      "  Validation Loss: 1.35\n",
      "  Validation took: 0:00:12\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of  1,000.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  1,000.    Elapsed: 0:00:23.\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_mcat = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       extra_data = b_mcat,\n",
    "                       labels=b_labels)\n",
    "\n",
    "        loss = result['loss']\n",
    "        logits = result['logits']\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_mcat = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           extra_data = b_mcat,\n",
    "                           labels=b_labels)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result['loss']\n",
    "        logits = result['logits']\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "# pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap (doesn't seem to work in Colab).\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader to batch our test samples for us. We'll use a sequential\n",
    "# sampler this time--don't need this to be random!\n",
    "prediction_sampler = SequentialSampler(test_dataset)\n",
    "prediction_dataloader = DataLoader(test_dataset, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_dataset)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_mcat, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      result = model(b_input_ids, \n",
    "                        attention_mask=b_input_mask,\n",
    "                        extra_data = b_mcat,\n",
    "                        labels=b_labels)\n",
    "\n",
    "  logits = result[\"logits\"]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate the F1\n",
    "f1 = f1_score(flat_true_labels, flat_predictions, average='micro')\n",
    "\n",
    "print('F1 Score: %.3f' % f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "878f638c0221e3ed571cdde73b54e3475bde5ecfcad9b387fc5f53bd43ed1e31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
